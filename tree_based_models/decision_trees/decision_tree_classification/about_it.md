# Decision trees as classification
- When the target variable is categorical (e.g., “spam” vs “not spam”), decision trees are used for classification.
    - ID3 algorithm builds trees using Information Gain based on entropy.
    - C4.5 improves on ID3 by handling continuous attributes, missing values, and pruning, and it uses Gain Ratio as its splitting metric.
    - CART (Classification mode) is another algorithm that uses the Gini Index to measure impurity. In all cases, the goal is to choose splits that maximize class purity in the resulting subsets.
### NOTE: images regarding this explanation will be inside the decision_tree_classification_images
## The whole workflow in the model using a example 
- lets take a dataset with 5 attributes (columns) 'outlook', 'temp', 'humidity', 'wind' and lastly 'play tennis' (target attribute).
- we need to know all the information gains of the attributes, the attribute having the highest info gain will be the root node, because it decides the decision tree.
- to calculate the info gain of the attribute, the first thing we need to calculate the entropy of the whole dataset, and the entropy of the each individual value of that attribute. for example: attribute=outlook, values of attribute are (sunny, overcast, rain), now we need to calculate the entropy of the whole dataset, entropy of the sunny, entropy of overcast and entropy of rain. then we will get info gain of the attribute=outlook. **see the decision_tree1.png image from the decision_tree_classification_images folder of this repo at this point**.
- now take all the info gains of the attribute at a place and see which is the highest, and take the attribute as the root node. **see the decision_tree2.png image from the decision_tree_classification_images folder for understading this point**
    - for example lets take as you saw the image, at the end outlook has the highest info gain, now it'll be the root node and the leaf node of that root will be the three values as branches,**you can see that in the image naming decision_tree3.png** in the images folder.
    - a point to note, we need to also note the target variables of the each branch, if you notice the table, you will know the for the branch overcast all the target  variables are yes, **which means if the outlook is overcast the _chance of playing tennis (which is the actual target caloumn(you can see that in the table))_ will be yes for sure without any doubt.** but if there is a mix of yes and no, it should be decided using some math internally, **you can see that in the image naming decision_tree3.png** in the images folder.
- now as there are two node which should be decide as yes or no, we need to claculate them by taking the number of valuse that branch had in that table, like sunny had total 5 attributes (2 as yes and 3 as no), and rain has 5 attribute (3 as yes and 2 as no), we to consider those atttributes and do that calculation. now lets take the sunny as first, lets consider that 5 attributes of that sunny branch, and no this time as we took the outlook first, we dont take it again, now we that the remaining attributes temp, humidity and wind, and calculate the same info gain of the three attribute there and do the same procees as we did here. **you can see decision_tree4.png to see that**, you can see there humidity has more info gain. now observe the values of the humidity in the table, you can see that if the humidity is high we have NO and if normal it's YES, now simply the sunny branch should have a decision as humidiy attribute and if the humidiy is high, the answe will be no and if normal the answer would be yes. **see the decision_tree5.png**
- take for the rain branch, wind attribute will have the highest info gain, and the values are weak and strong, if we observe, we can clearly see, if wind is weak the answer will be yes, and if it's strong the answer will be no. the tree is completed. NOTE: im not mentioning the info gains of the attributes here, beacuse we alrealy understood the process, im showing the final tree now.
- **see the decision_tree6.png for the final tree**